apiVersion: batch/v1
kind: Job
metadata:
  name: unetr-brats-ft-job
  labels:
    app: unetr-brats-ft
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: unetr-brats-ft
    spec:
      nodeSelector:
        topology.kubernetes.io/region: us-west
        nautilus.io/linstor: "true"
      containers:
        - name: brats-processing
          image: gitlab-registry.nrp-nautilus.io/prp/jupyter-stack/prp
          env:
            - name: REPO_PATH
              value: /app/UNETR-BraTS-Synthesis
            - name: PYTHONPATH
              value: /app/UNETR-BraTS-Synthesis
            # FIX: Set to single GPU since we're only requesting 1
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # nnUNet paths
            - name: nnUNet_raw
              value: /app/nnunet/raw
            - name: nnUNet_preprocessed
              value: /app/nnunet/preprocessed
            - name: nnUNet_results
              value: /app/nnunet/results
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: PYTHONIOENCODING
              value: "UTF-8"
            # W&B configuration
            - name: WANDB_API_KEY
              valueFrom:
                secretKeyRef:
                  name: wandb-credentials-edu
                  key: api-key
            - name: WANDB_PROJECT
              value: "UNETR-BraTS-SegFinetuneToSynth"
            - name: WANDB_ENTITY
              value: "BraTS2025"
            - name: WANDB_MODE
              value: "online"
            - name: SYNAPSE_AUTHTOKEN
              valueFrom:
                secretKeyRef:
                  name: synapse-credentials
                  key: authtoken
          command: ["/bin/bash", "-c"]
          args:
            - |
              # Add GPU debugging at the start
              echo "=== GPU DEBUG INFO ==="
              nvidia-smi || echo "nvidia-smi not available"
              echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
              
              # pip install synapseclient
              # synapse config
              # synapse get -r syn51514105 

              git clone -b fixval https://github.com/tsereda/UNETR-BraTS-Synthesis.git ${REPO_PATH}
              cd ${REPO_PATH}
              
              sudo apt-get update && sudo apt-get install -y p7zip-full wget git
              
              for dataset in "TrainingData" "ValidationData"; do
                zip_file="/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}.zip"
                if [ -f "$zip_file" ]; then
                  echo "Extracting ${dataset}..."
                  cd /data
                  # Extract directly to /data with proper directory structure
                  sudo 7z x "$zip_file" -o"/data/"
                  # Fix ownership
                  if [ -d "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}" ]; then
                    sudo chown -R jovyan:jovyan "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}"
                    echo "Successfully extracted and set ownership for ${dataset}"
                  else
                    echo "Warning: Expected directory /data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset} not found after extraction"
                    echo "Contents of /data after extraction:"
                    ls -la /data/
                  fi
                  cd ${REPO_PATH}
                  ln -sf "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}" .
                else
                  echo "Zip file not found: $zip_file"
                fi
              done
              
              pip install "numpy<2.0" pyyaml torch tqdm nibabel wandb matplotlib monai[all]
              pip install --upgrade tensorboard protobuf typing_extensions
              
              python -c "import wandb; print(f'W&B version: {wandb.__version__}')"
              
              # Enhanced GPU availability check
              python -c "
              import torch
              print(f'PyTorch version: {torch.__version__}')
              print(f'CUDA available: {torch.cuda.is_available()}')
              print(f'CUDA version: {torch.version.cuda}')
              print(f'GPUs available: {torch.cuda.device_count()}')
              if torch.cuda.is_available():
                  for i in range(torch.cuda.device_count()):
                      print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
              "
              
              pip install "numpy<2.0" torch nibabel matplotlib monai[all]

              cp /data/segresults/best_model_batchsize4_epoch100_128.pt .

              # Create a patch for the CUDA issue
              cat > fix_cuda.py << 'EOF'
              import sys
              
              # Read the original file
              with open('ft_synth.py', 'r') as f:
                  content = f.read()
              
              # Apply fixes
              fixes = [
                  # Fix device detection
                  ('    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")', 
                   '''    # Enhanced device detection and setup
                   if not torch.cuda.is_available():
                       print('WARNING: CUDA not available, using CPU')
                       device = torch.device('cpu')
                   else:
                       print(f'CUDA available: {torch.cuda.device_count()} GPUs')
                       for i in range(torch.cuda.device_count()):
                           print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
                       device = torch.device('cuda')
                   print(f'Using device: {device}')'''),
                  
                  # Replace .cuda() calls with .to(device)
                  ('.cuda()', '.to(device)'),
                  
                  # Fix input_data and target_data cuda calls in training functions
                  ('input_data = batch_data["input_image"].cuda()', 'input_data = batch_data["input_image"].to(device)'),
                  ('target_data = batch_data["target_image"].cuda()', 'target_data = batch_data["target_image"].to(device)'),
              ]
              
              for old, new in fixes:
                  content = content.replace(old, new)
              
              # Write the fixed file
              with open('ft_synth_fixed.py', 'w') as f:
                  f.write(content)
              
              print("âœ“ Created ft_synth_fixed.py with CUDA fixes")
              EOF
              
              python fix_cuda.py
              
              # Run with enhanced error handling
              python ft_synth.py --pretrained_path best_model_batchsize4_epoch100_128.pt --save_path 10_flair.pt --max_epochs 10 --target_modality FLAIR
              ls -la
              sudo cp 10_flair.pt /data/
              ls -la /data/
              python ft_synth.py --pretrained_path best_model_batchsize4_epoch100_128.pt --save_path 10_t1ce.pt --max_epochs 10 --target_modality T1CE
              python ft_synth.py --pretrained_path best_model_batchsize4_epoch100_128.pt --save_path 10_t1.pt --max_epochs 10 --target_modality T1
              python ft_synth.py --pretrained_path best_model_batchsize4_epoch100_128.pt --save_path 10_t2.pt --max_epochs 10 --target_modality T2
              sudo cp 10_t1ce.pt /data/
              sudo cp 10_t1.pt /data/
              sudo cp 10_t2.pt /data/
              ls -la /data/

              # Run inference
              # python inferenceseg.py \
              #   --model_path best_model_batchsize4_epoch100_128.pt \
              #   --subset 5 \
              #   --output_dir .
          volumeMounts:
            - name: workspace
              mountPath: /app
            - name: data
              mountPath: /data
            - name: shm
              mountPath: /dev/shm
          resources:
            requests:
              memory: 25Gi
              cpu: "15"
              nvidia.com/a100: "1"
            limits:
              memory: 30Gi
              cpu: "18"
              nvidia.com/a100: "1"
      volumes:
        - name: workspace
          emptyDir:
            sizeLimit: 50Gi
        - name: data
          persistentVolumeClaim:
            claimName: brats2025-2
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      restartPolicy: Never