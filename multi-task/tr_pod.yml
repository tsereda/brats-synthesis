apiVersion: v1
kind: Pod
metadata:
  name: tr-multitask
  labels:
    app: tr-multitask
spec:
  nodeSelector:
    #topology.kubernetes.io/region: us-west
    nautilus.io/linstor: "true"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.product
            operator: In
            values:
            - NVIDIA-A10
            - NVIDIA-GeForce-RTX-3090
            - NVIDIA-GeForce-RTX-4090
            - NVIDIA-TITAN-RTX
            - NVIDIA-RTX-A5000
            - Quadro-RTX-6000
            - Tesla-V100-SXM2-32GB
            - NVIDIA-A40
            - NVIDIA-L40
            - NVIDIA-RTX-A6000
            - Quadro-RTX-8000
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
    - key: nvidia.com/gpu
      operator: Exists
      effect: PreferNoSchedule
  containers:
    - name: brats-processing
      image: gitlab-registry.nrp-nautilus.io/prp/jupyter-stack/prp
      env:
        - name: REPO_PATH
          value: /app/brats-synthesis
        - name: PYTHONPATH
          value: /app/brats-synthesis
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: PYTHONIOENCODING
          value: "UTF-8"
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: wandb-credentials-edu
              key: api-key
        - name: WANDB_PROJECT
          value: "UNETR-BraTS-SegFinetuneToSynth"
        - name: WANDB_ENTITY
          value: "timgsereda"
        - name: WANDB_MODE
          value: "online"
      command: ["/bin/bash", "-c"]
      args:
        - |
          sudo mkdir /data/multitask_models/
          sudo chmod 777 /data/multitask_models/
          git clone https://github.com/tsereda/brats-synthesis.git ${REPO_PATH}
          cd ${REPO_PATH}
          
          sudo apt-get update && sudo apt-get install -y p7zip-full wget git
          
          for dataset in "TrainingData" "ValidationData"; do
            zip_file="/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}.zip"
            if [ -f "$zip_file" ]; then
              echo "Extracting ${dataset}..."
              cd /data
              # Extract directly to /data with proper directory structure
              sudo 7z x "$zip_file" -o"/data/"
              # Fix ownership
              if [ -d "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}" ]; then
                sudo chown -R jovyan:jovyan "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}"
                echo "Successfully extracted and set ownership for ${dataset}"
              else
                echo "Warning: Expected directory /data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset} not found after extraction"
                echo "Contents of /data after extraction:"
                ls -la /data/
              fi
              cd ${REPO_PATH}
              ln -sf "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}" .
            else
              echo "Zip file not found: $zip_file"
            fi
          done
          
          pip install "numpy<2.0" pyyaml tqdm nibabel wandb matplotlib monai[all]
          pip install --upgrade tensorboard protobuf typing_extensions
          
          python -c "import wandb; print(f'W&B version: {wandb.__version__}')"
          
          # python scripts/test_wandb.py
          
          python -c "import torch; print(f'GPUs available: {torch.cuda.device_count()}')"
          
          pip install "numpy<2.0" torch nibabel matplotlib monai[all]


          python multi-task/train.py --batch_size 1 --max_epoch 2 --target_modality all

          # Run inference
          # python inferenceseg.py \
          #   --model_path best_model_batchsize4_epoch100_128.pt \
          #   --subset 5 \
          #   --output_dir .

          tail -f /dev/null
      volumeMounts:
        - name: workspace
          mountPath: /app
        - name: data
          mountPath: /data
        - name: shm
          mountPath: /dev/shm
      resources:
        requests:
          memory: 24Gi  
          cpu: "12"              
          nvidia.com/gpu: "1"
        limits:
          memory: 32Gi
          cpu: "16"
          nvidia.com/gpu: "1"
  volumes:
    - name: workspace
      emptyDir:
        sizeLimit: 50Gi
    - name: data
      persistentVolumeClaim:
        claimName: brats2025-4
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi
  restartPolicy: Never